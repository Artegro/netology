# Домашнее задание к занятию 17 «Инцидент-менеджмент»

## Задание

Составьте постмортем на основе реального сбоя системы GitHub в 2018 году.

Информацию о сбое можно изучить по ссылкам ниже:

* [краткое описание на русском языке](https://habr.com/ru/post/427301/);
* [развёрнутое описание на английском языке](https://github.blog/2018-10-30-oct21-post-incident-analysis/).


## Ответ

### Собственно сам постмортем , по сути это хроника событий и дествий, то есть протокол событий, погнали:
###  Краткое описание

-   21 октября В 22:52 Деградация работы сервиса и снижение качества обслуживания в течение 24 часов и 11 минут.

### Предшествующие события 

-   Плановые работы по техническому обслуживанию для замены вышедшего из строя оптического оборудования 100G привели к потере связи между нашим сетевым узлом и основным центром обработки данных на Восточном побережье США

### Причина инцедента 

-   Потере связи между сетевым узлом на Восточном побережье (US East Coast) и основным дата-центром на Восточном побережье на 43 секунды,

### Воздействие

-   Отключение восточного ЦОД от централногопривело к перестройки оркестратором на использование баз данных западного ЦОД как основного. но в это время в восточном цод так же паралельна шла в базы восточного , что привело к несогласованности данных после востановления связи.

### Обнаружение

-   Сообшения о сбоях были обнаруженны инженерами над сортировкой входящих уведомлений. Далее была привлечена группа быстрого реагирования и координатор инцидентов.

### Реакция

-   Ответственная группа восстановила работоспособность за 24 часа, сознательно пожертовав качеством предоставляемых сервисов в угоду целосности данных

### Восстановление

-   Были восстановленны кластеры базы данных в отдельных ЦОД по очереди и возабновленны репликации, дабы избежать конфликта данных. Работоспособность сервисов полностью востановлена без потерь данных.

### Таймлайн
**2018 Октябрь 21 22:52**

Во время описанного выше разделения сети Orchestrator, начала процесс смены лидера, в соответствии с консенсусом Raft. ЦОД на Западном побережье и узлы Orchestrator в общедоступном облаке на Восточном смогли создать кворум и начать отработку отказа кластеров для направления записи в Западный ЦОД. Orchestrator приступил к организации кластерных топологий баз данных Западного побережья. Когда подключение было восстановлено, трафик пошел на новые посновные серверы на Западе.

Серверы баз данных Восточном ЦОД-е содержали записи за период резделения сети, который не был реплицирован на объект в Западном ЦОД-е. Поскольку кластеры баз данных в обоих центрах обработки данных теперь содержали операции записи, которых не было в другом центре обработки данных, нам не удалось безопасно перенести первичный сервер обратно в Восточный ЦОД.

**2018 Октябрь 21 22:54**

Внутренний мониторинг начал генерировать оповещения, указывающие на многочисленные сбои в наших системах. В это время несколько инженеров отвечали и работали над сортировкой входящих уведомлений. К 23:02 инженеры нашей группы быстрого реагирования определили, что топологии для многочисленных кластеров баз данных находятся в неожиданном состоянии. При запросе API Orchestrator отображалась топология репликации базы данных, которая включала только серверы из западного ЦОД.

**2018 Октябрь 21 23:07**

К этому моменту группа реагирования решила вручную заблокировать внутренние средства развёртывания, чтобы предотвратить внесение дополнительных изменений. В 23:09 группа установила жёлтый статус работоспособности сайта, что автоматически присвоило ситуации статус активного инцидента и отправило предупреждение координатору инцидентов. В 23:11 координатор присоединился к работе и через две минуты принял решение изменить статус на красный.

**21.10.2018, 23:13**

На тот момент было понятно, что проблема затрагивает несколько кластеров БД. К работе привлекли дополнительных разработчиков из инженерной группы БД. Они начали исследовать текущее состояние, чтобы определить, какие действия необходимо предпринять, чтобы вручную настроить базу данных Восточного побережья США в качестве основной для каждого кластера и перестроить топологию репликации. Это было непросто, поскольку к этому моменту западный кластер баз данных принимал записи с уровня приложений в течение почти 40 минут. Кроме того, в восточном кластере существовало несколько секунд записей, которые не были реплицированы на запад и не позволяли реплицировать новые записи обратно на восток.

Защита конфиденциальности и целостности пользовательских данных является наивысшим приоритетом GitHub. Поэтому мы решили, что более 30 минут данных, записанных в западный ЦОД, оставляют нам только один вариант решения ситуации, чтобы сохранить эти данные: перенос вперёд (failing-forward). Однако приложения на востоке, которые зависят от записи информации в западный кластер MySQL, в настоящее время не способны справиться с дополнительной задержкой из-за передачи большинства их вызовов БД туда и обратно. Это решение приведёт к тому, что наш сервис станет непригодным для многих пользователей. Мы считаем, что длительная деградация качества обслуживания стоила того, чтобы обеспечить согласованность данных наших пользователей.

**21.10.2018, 23:19**

Запросы о состоянии кластеров БД показали, что необходимо остановить выполнение заданий, которые пишут метаданные типа пуш-запросов. Мы сделали выбор и сознательно пошли на частичную деградацию сервиса, приостановив вебхуки и сборку GitHub Pages, чтобы не ставить под угрозу данные, которые уже получили от пользователей. Другими словами, стратегия заключалась в расстановке приоритетов: целостность данных вместо удобства использования сайта и быстрого восстановления.

**22.10.2018, 00:05**

Инженеры из группы реагирования начали разрабатывать план устранения несогласованности данных и запустили процедуры отработки отказа для MySQL. План состоял в том, чтобы восстановить файлы из бэкапа, синхронизировать реплики на обоих сайтах, вернуться к стабильной топологии обслуживания, а затем возобновить обработку заданий в очереди. Мы обновили статус, чтобы сообщить пользователям, что собираемся выполнить управляемую отработку отказа внутренней системы хранения данных.

Хотя резервные копии MySQL делаются каждые четыре часа и сохраняются в течение многих лет, но они лежат в удалённом облачном хранилище блоб-объектов. Восстановление нескольких терабайт из бэкапа заняло несколько часов. Долго шла передача данных из службы удалённого резервного копирования. Основная часть времени ушла на процесс распаковки, проверки контрольной суммы, подготовки и загрузки больших файлов бэкапа на свежеподготовленные серверы MySQL. Эта процедура тестируется ежедневно, поэтому все хорошо представляли, сколько времени займёт восстановление. Однако до этого инцидента нам никогда не приходилось полностью перестраивать весь кластер из резервной копии. Всегда работали другие стратегии, такие как отложенные реплики.

**22.10.2018, 00:41**

К этому времени был инициирован процесс резервного копирования для всех затронутых кластеров MySQL, и инженеры отслеживали прогресс. Одновременно несколько групп инженеров изучали способы ускорения передачи и восстановления без дальнейшей деградации сайта или риска повреждения данных.

**22.10.2018, 06:51**

Несколько кластеров в восточном ЦОД завершили восстановление из резервных копий и начали реплицировать новые данные с Западным побережьем. Это привело к замедлению загрузки страниц, которые выполняли операцию записи через всю страну, но чтение страниц из этих кластеров БД возвращало актуальные результаты, если запрос чтения попадал на только что восстановленную реплику. Другие более крупные кластеры БД продолжали восстанавливаться.

Наши команды определили способ восстановления непосредственно с Западного побережья, чтобы преодолеть ограничения пропускной способности, вызванные загрузкой из внешнего хранилища. Стало практически на 100% понятно, что восстановление завершится успешно, а время для создания здоровой топологии репликации зависит от того, сколько займёт догоняющая репликация. Эта оценка была линейно интерполирована на основе доступной телеметрии репликации, и страница состояния была обновлена, чтобы установить ожидание в два часа в качестве расчётного времени восстановления

**22.10.2018, 07:46**

GitHub опубликовал информационное сообщение в блоге. Мы сами используем GitHub Pages, и все сборки поставили на паузу несколько часов назад, поэтому публикация потребовала дополнительных усилий. Приносим извинения за задержку. Мы намеревались разослать это сообщение гораздо раньше и в будущем обеспечим публикации обновлений в условиях таких ограничений.

**22.10.2018, 11:12**

Все первичные БД вновь переведены на Восток. Это привело к тому, что сайт стал гораздо более отзывчивым, так как записи теперь были направлялись на сервер БД, расположенный в том же физическом ЦОД, что и наш уровень приложений. Хотя это существенно повысило производительность, всё ещё остались десятки реплик чтения БД, которые отставали от основной копии на несколько часов. Эти отложенные реплики привели к тому, что пользователи видели несогласованные данные при взаимодействии с нашими службами. Мы распределяем нагрузку чтения по большому пулу реплик чтения, и каждый запрос к нашим службам имеет хорошие шансы попасть в реплику чтения с задержкой в несколько часов.

На самом деле время догона отстающей реплики сокращается по экспоненте, а не линейно. Когда проснулись пользователи в США и Европе, из-за увеличения нагрузки на записи в кластерах БД процесс восстановления занял больше времени, чем предполагалось.

**22.10.2018, 13:15**

Мы приближались к пику нагрузки на GitHub.com. В команде реагирования прошло обсуждение дальнейших действий. Было ясно, что отставание репликации до согласованного состояния увеличивается, а не уменьшается. Ранее мы начали подготовку дополнительных реплик чтения MySQL в общедоступном облаке Восточного побережья. Как только они стали доступны, стало легче распределять поток запросов на чтение между несколькими серверами. Уменьшение средней нагрузки на реплики чтения ускорило догон репликации.

**22.10.2018, 16:24**

После синхронизации реплик мы вернулись к исходной топологии, устранив проблемы задержки и доступности. В рамках сознательного решения о приоритете целостности данных над быстрым исправлением ситуации мы сохранили красный статус сайта, когда начали обрабатывать накопленные данные.

**22.10.2018, 16:45**

На этапе восстановления нужно было сбалансировать возросшую нагрузку, связанную с отставанием, потенциально перегружая наших партнёров по экосистеме уведомлениями и как можно быстрее возвращаясь к стопроцентной работоспособности. В очереди оставалось более пяти миллионов событий хуков и 80 тыс. запросов на построение веб-страниц.

Когда мы повторно включили обработку этих данных, то обработали около 200 000 полезных задач с вебхуками, которые превысили внутренний TTL и были отброшены. Узнав об этом, мы остановили обработку и запушили увеличение TTL.

Чтобы избежать дальнейшего снижения надёжности наших обновлений статуса, мы оставили статус деградации до тех пор, пока не завершим обработку всего накопившегося объёма данных и не убедимся, что сервисы чётко вернулись к нормальному уровню производительности.

**22.10.2018, 23:03**

Все незавершённые события вебхуков и сборки Pages обработаны, а целостность и правильная работа всех систем подтверждена. Статус сайта обновлён на зелёный.

#### Последующие действия

- Отрегулировать конфигурацию Orchestrator
- ускорили миграцию на новую систему отчётности по статусам
- обеспечение резервирования на уровне ЦОД N+1
- передача знаний коллегам в виду усложнения системы
- дополнительные проверки сценариев сбоев - эмуляция, отработка аварий и аварийных ситуаций

